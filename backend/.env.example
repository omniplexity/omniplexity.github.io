# OmniAI Backend Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# SERVER
# =============================================================================
# Use 0.0.0.0 inside Docker; use 127.0.0.1 for local non-Docker runs
HOST=0.0.0.0
PORT=8000
DEBUG=false
LOG_LEVEL=INFO
# Optional log file path (Docker default: /app/logs/omniai.log)
LOG_FILE=

# =============================================================================
# SECURITY
# =============================================================================
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
SECRET_KEY=REPLACE_WITH_64_CHAR_SECRET

# CORS allowed origins (comma-separated)
CORS_ORIGINS=https://your-frontend.example.com

# Rate limiting (requests per minute per IP)
RATE_LIMIT_RPM=60

# Max request body size in bytes (default 1MB)
MAX_REQUEST_BYTES=1048576

# =============================================================================
# AUTHENTICATION
# =============================================================================
# Session cookie settings
SESSION_COOKIE_NAME=omni_session
SESSION_TTL_SECONDS=604800
COOKIE_SECURE=true
COOKIE_SAMESITE=lax
COOKIE_DOMAIN=

# CSRF settings
CSRF_HEADER_NAME=X-CSRF-Token
CSRF_COOKIE_NAME=omni_csrf

# Registration settings
INVITE_REQUIRED=true

# =============================================================================
# DATABASE
# =============================================================================
# SQLite (default): sqlite:///./data/omniai.db
# PostgreSQL: postgresql://user:pass@localhost:5432/omniai
DATABASE_URL=sqlite:///./data/omniai.db

# =============================================================================
# PROVIDERS (configure as needed)
# =============================================================================
# Default provider (lmstudio, ollama, or openai_compat)
PROVIDER_DEFAULT=lmstudio

# Comma-separated list of enabled providers
PROVIDERS_ENABLED=lmstudio

# Provider timeouts and retries
PROVIDER_TIMEOUT_SECONDS=30
PROVIDER_MAX_RETRIES=1
# Server-sent events keep-alive interval (seconds; set to 0 to disable)
SSE_PING_INTERVAL_SECONDS=10

# LM Studio (OpenAI-compatible local server)
# In Docker, use host.docker.internal to reach your host machine
LMSTUDIO_BASE_URL=http://host.docker.internal:1234

# Ollama (local LLM server)
# In Docker, use host.docker.internal to reach your host machine
OLLAMA_BASE_URL=http://host.docker.internal:11434

# OpenAI-compatible endpoint (generic)
OPENAI_COMPAT_BASE_URL=https://openai-compat.example.com/v1
OPENAI_COMPAT_API_KEY=REPLACE_WITH_OPENAI_COMPAT_API_KEY
