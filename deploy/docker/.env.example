# OmniAI Docker Production Environment
# Copy to .env and fill in values. NEVER commit .env!

# =============================================================================
# NGROK TUNNEL (REQUIRED)
# =============================================================================
# Get your auth token from: https://dashboard.ngrok.com/get-started/your-authtoken
NGROK_AUTHTOKEN=

# =============================================================================
# AUTHENTICATION (REQUIRED - generate new values!)
# =============================================================================
# python -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=

# python -c "import secrets; print(secrets.token_hex(16))"
CSRF_SECRET=

# One-time admin bootstrap token (use once, then delete from .env)
# python -c "import secrets; print(secrets.token_urlsafe(32))"
ADMIN_BOOTSTRAP_TOKEN=

# =============================================================================
# ORIGIN LOCK (REQUIRED for tunnel security)
# =============================================================================
ORIGIN_LOCK_ENABLED=true

# Shared secret - ngrok injects this as X-Origin-Secret header automatically
# python -c "import secrets; print(secrets.token_urlsafe(32))"
ORIGIN_LOCK_SECRET=

# =============================================================================
# COOKIES (REQUIRED for cross-origin GitHub Pages)
# =============================================================================
COOKIE_SECURE=true
COOKIE_SAMESITE=None
SESSION_COOKIE_NAME=omniplexity_session
SESSION_TTL_SECONDS=604800

# =============================================================================
# CORS (REQUIRED - add your ngrok URL after first startup)
# =============================================================================
# 1. Start with just GitHub Pages origin
# 2. Run `docker compose logs ngrok` to get your tunnel URL
# 3. Add the ngrok URL to this list
# 4. Run `docker compose restart backend`
CORS_ORIGINS=["https://omniplexity.github.io"]

# =============================================================================
# SERVER
# =============================================================================
LOG_LEVEL=INFO
ENVIRONMENT=production
INVITE_ONLY=true

# =============================================================================
# UVICORN RUNTIME (OPTIONAL)
# =============================================================================
# NOTE: Keep workers=1 unless you are certain multi-process won't break
# in-memory cancellation/rate limiting behaviors.
RUN_MIGRATIONS=1
UVICORN_WORKERS=1
UVICORN_LOG_LEVEL=info

# =============================================================================
# LLM PROVIDERS (use host.docker.internal for host services)
# =============================================================================
# LM Studio running on host machine
LMSTUDIO_BASE_URL=http://host.docker.internal:1234/v1
LMSTUDIO_TIMEOUT_SECONDS=120

# Ollama running on host machine
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_TIMEOUT_SECONDS=120

# OpenAI-compatible API (optional)
OPENAI_COMPAT_BASE_URL=
OPENAI_API_KEY=
OPENAI_TIMEOUT_SECONDS=120

# =============================================================================
# SSE STREAMING
# =============================================================================
SSE_HEARTBEAT_SECONDS=15
SSE_CLIENT_DISCONNECT_GRACE_SECONDS=2

# =============================================================================
# MEMORY (CHROMA VECTOR STORE)
# =============================================================================
MEMORY_ENABLED=true
MEMORY_CHROMA_PATH=/app/data/chroma
MEMORY_COLLECTION=omni_memory
MEMORY_TOP_K=6
MEMORY_MIN_SCORE=0.2
MEMORY_MAX_CHARS=1200
MEMORY_AUTO_INGEST_USER_MESSAGES=true
MEMORY_AUTO_INGEST_ASSISTANT_MESSAGES=false
MEMORY_EMBEDDING_BACKEND=auto
MEMORY_EMBEDDING_MODEL=text-embedding-3-small
MEMORY_EMBEDDING_BASE_URL=
MEMORY_EMBEDDING_API_KEY=
